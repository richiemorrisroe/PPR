#+TITLE: Property Analysis
#+HTML_HEAD: <style type="text/css">.example {background-color: #eff0f1; color: #ff0000;}</style>
#+OPTIONS: toc:nil
#+OPTIONS: ^:nil
#+SETUPFILE: ~/Dropbox/org-minimal-html-theme/org-minimal-theme.setup
#+PROPERTY: header-args: :exports code :eval no
#+PROPERTY: header-args:R :session *R* :eval no-export

* Data Munging :noexport:
#+BEGIN_SRC elisp :results none
(setq org-html-htmlize-output-type 'css)
#+END_SRC

#+RESULTS:
: css

#+BEGIN_SRC R :session :results none :exports none
require(ggplot2)
require(reshape2)
require(lubridate)
require(dplyr)
require(tidyverse)
require(readxl)
#+END_SRC

#+RESULTS:
: TRUE
#+BEGIN_SRC R :session :results none :exports code
dat <- read_excel("PPR-ALL.xlsx", sheet="PPR-ALL")
normalise_names <- function(df) {
    nms <- names(df)
    normed <- iconv(tolower(gsub("([[:space:]]|[[:punct:]])+", "_", x=nms)), "latin1", "ASCII", sub="")
    drop_usc <- gsub("([a-z_])_*$", "\\1", x=normed)
    names(df) <- drop_usc
    df
}
fix_price <- function(x) {
    nopunct <- gsub(",|\\.", "", x=x)
    nums <- iconv(nopunct, "latin1", "ASCII", sub="")
}
dat2 <- normalise_names(dat)
dat2$price <- with(dat2, fix_price(price))
prop_df <- dat2 %>%
    mutate(date_of_sale=lubridate::dmy(date_of_sale_dd_mm_yyyy),
           not_full_market_price=as.factor(not_full_market_price),
           postal_code=as.factor(fct_explicit_na(postal_code)),
           vat_exclusive=as.factor(vat_exclusive),
           county=as.factor(county),
           price=as.numeric(price)/100)
prop_df2 <- mutate(prop_df, year=lubridate::year(date_of_sale), month=lubridate::month(date_of_sale, label=TRUE), day=lubridate::day(date_of_sale))
train <- filter(prop_df2, year<=2013)
train_sample <- sample_frac(train, size=0.1)
train_sample2 <- (filter(train_sample, price<1e6))
select(train_sample2, 1, address, county) %>% write_csv("train_sample2.csv", na="")
#+END_SRC

#+BEGIN_SRC R :session :results none :eval no :exports code
ireland <- readr::locale(date_names = "en", date_format = "%AD", time_format = "%AT",
       decimal_mark = ".", grouping_mark = ",", tz = "UTC",
       encoding = "windows-1252", asciify = FALSE)
prop <- lapply(X=list.files(pattern="^PPR.*.csv"), function (x) readr::read_csv(x, locale=ireland))
propdf <- dplyr::bind_rows(prop)
names(propdf) <- c("date_sale", "address", "postal_code", "county", "price", "not_full_market_price", "vat_exclusive", "description", "property_size")
propdf2 <- propdf[,1:7]
prop_non_num <- prop[c(1:5,8)] %>% bind_rows()
names(prop_non_num) <- c("date_sale", "address", "postal_code", "county", "price", "not_full_market_price", "vat_exclusive", "description", "property_size")
prop_ok_num <- prop[c(6:7)] %>% bind_rows()
names(prop_ok_num) <- c("date_sale", "address", "postal_code", "county", "price", "not_full_market_price", "vat_exclusive", "description", "property_size")
prop_ok_num <-  mutate(prop_ok_num, date_sale=dmy(date_sale))

ppr_df_nonnum <-  prop_non_num %>% mutate(date_sale=dmy(date_sale), price2=as.numeric(gsub(pattern="[^0-9]+", replacement="", x=price))/100)

ppr_df_nonnum2 <-  select(ppr_df_nonnum, 1:8, price=price2)
ppr_df <-  bind_rows(ppr_df_nonnum2, prop_ok_num)
sales_yearly <- ppr_df %>% mutate(year=year(date_sale)) %>% group_by(year) %>% summarise(sales=length(address), med_price=median(price), avg_price=mean(price, na.rm=TRUE))
dub_sales_yearly <- ppr_df %>% filter(county=="Dublin") %>% mutate(year=year(date_sale), month=month(date_sale)) %>% group_by(date_sale) %>% summarise(sales=length(address), med_price=median(price), avg_price=mean(price, na.rm=TRUE))
county_sales_yearly <- ppr_df %>% mutate(year=year(date_sale), month=month(date_sale), county=county) %>% group_by(county, date_sale) %>% summarise(sales=length(address), med_price=median(price), avg_price=mean(price, na.rm=TRUE))
#+END_SRC
# so it was much easier when the PPR people added a single file
# iconv is great, but this is a terrible hack and I should be ashamed

#+BEGIN_SRC R :session :results none :eval no
  ggplot(sales_yearly, aes(x=year, y=med_price))+geom_line()
  ggplot(ppr_df, aes(x=year(date_sale), y=price))+geom_bar(stat="identity")+facet_grid(.~year)
  ggplot(sales_yearly, aes(x=year, y=sales))+geom_line()
  ggplot(sales_yearly, aes(x=year, y=med_price))+geom_line()
  ggplot(sales_yearly, aes(x=year, y=avg_price))+geom_line()
  ggplot(ppr_df, aes(x=year(date_sale), y=price))+geom_bar(stat="identity")+facet_grid(.~year)
  ggplot(ppr_df, aes(x=year(date_sale), y=price))+geom_bar(stat="identity")
  ggplot(ppr_df, aes(x=price))+geom_bar(stat="identity")+facet_grid(.~year(date_sale))
  ggplot(ppr_df2, aes(x=price))+geom_bar(stat="identity")+facet_grid(.~year)
  ggplot(ppr_df2, aes(x=price))+geom_bar()+facet_grid(.~year)
  ggplot(ppr_df, aes(x=price))+geom_density()
  ggplot(sales_yearly, aes(x=year, y=med_price))+geom_line()
  ggplot(dub_sales_yearly, aes(x=year, y=med_price))+geom_line()
  ggplot(dub_sales_yearly, aes(x=month, y=med_price, group=year))+geom_line()
  ggplot(dub_sales_yearly, aes(x=month, y=med_price, colour=year))+geom_line()
  ggplot(dub_sales_yearly, aes(x=month, y=med_price, colour=year, group=year))+geom_line()
  ggplot(dub_sales_yearly, aes(x=month, y=med_price, colour=as.factor(year), group=year))+geom_line()
  ggplot(dub_sales_yearly, aes(x=date_sale, y=med_price))+geom_line()
  ggplot(dub_sales_yearly, aes(x=date_sale, y=med_price))+geom_smooth()
  ggplot(dub_sales_yearly, aes(x=date_sale, y=med_price))+geom_smooth()+coord_cartesian(ylim=c(0, 350000))
  ggplot(cork_sales_yearly, aes(x=date_sale, y=med_price))+geom_smooth()+coord_cartesian(ylim=c(0, 350000))
  ggplot(county_sales_yearly, aes(x=date_sale, y=med_price, colour=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=med_price, colour=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=med_price, colour=county, size=sales))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=med_price, colour=county, size=sales))+geom_line()
  ggplot(county_sales_yearly, aes(x=date_sale, y=med_price, colour=county, size=sales, group=county))+geom_line()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, colour=county,group=county))+geom_line()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, fill=county,group=county))+geom_area()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales*price, colour=county,group=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, fill=county,group=county))+geom_area()
  ggplot(county_sales_yearly, aes(x=date_sale, y=med_price, colour=county,group=county))+geom_smooth()
  ggplot(county_sales_yearly, aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
  filter(county_sales_yearly, county!="Dublin" & country!="Cork") %>% ggplot(aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
  filter(county_sales_yearly, county!="Dublin" & county!="Cork") %>% ggplot(aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
  filter(county_sales_yearly, county!="Dublin" & county!="Cork") %>% ggplot(aes(x=date_sale, y=sales, fill=county,group=county))+geom_bar(stat="identity")
  filter(county_sales_yearly, county!="Dublin" & country!="Cork") %>% ggplot(aes(x=date_sale, y=sales, colour=county,group=county))+geom_smooth()
#+END_SRC

** TODO I can join this data with the CSO data, and figure out how much of the stock has changed hand

** TODO Scrape DAFT for more up-to-date information
** Geocoding

- Gonna do this in Python
- writing this while my stan code compiles :) 

#+BEGIN_SRC python :session :tangle geocode.py :exports code :eval no
import requests as r
import time
import pandas as pd
import json
import random
addresses = pd.read_csv("train_sample2.csv")
add_filled = addresses.fillna("")
add_filled.address = addresses.address.apply(lambda x: x.lower())
add = addresses.address
add2 = [x.lower() for x in add]
add3 = add2[0:10]
add4 = [x + ",ireland" for x in add3]
add_test = addresses.head()
geocode_results = []
failures = []
fail_content = []
for i, location in add_filled.iterrows():
    lloc = list(location)
    date, add1, add2 = lloc
    locstr = f"{add1},{add2},ireland"
    print(locstr)
    geocodestr = "https://geocode.xyz/?auth=623294118775338849269x1104&locate={locstr}&region=IE&geoit=json"
    req = r.get(geocodestr.format(locstr=locstr))
    sleeptime = random.randint(1, 10) % 10
    print(f"sleeping for {sleeptime} seconds")
    time.sleep(sleeptime)

    print(i)
    if req.status_code == 200:
        myjson = json.loads(req.content)
        print(myjson)
        geocode_results.append(myjson)
    else:
        failures.append(location)
        fail_content.append(json.loads(req.content))
        print(req.content)
    if i % 100==0:
        gc_df = pd.DataFrame(geocode_results)
        gc_df.to_csv("gc_results.csv")


    

#+END_SRC

- Damnit, requests throttled!
- i may need to pay them some money, unfortunately. 

After paying some money, I got a sample of geocoded houses. 

#+BEGIN_SRC python :session :results none :eval no
import pandas as pd
gc_res = pd.read_csv("gc_results.csv")
print(gc_res.shape)
res_good =  gc_res[gc_res.error.isnull()]
print(res_good.shape)
#+END_SRC
*** Grabbing someone else's work

- Shane Lynn did this before.

- We can read in his data and get a big head start

- only covers 2012-17

- but has latitude and longitude, and shapefiles, which will help a lot. 

#+BEGIN_SRC R :results none :exports code :eval no
require(sp)
require(rgdal)
require(tidyverse)
ppr_gc <- read_csv("ppr_data_encoded.csv")
ppr_gc_smaller <- select(ppr_gc, year, input_string, sale_date, price, ppr_county, geo_county, description_of_property, 15:24) %>% filter(price<1e6)
ppr_gc2 <- filter(ppr_gc_smaller, !is.na(latitude), !is.na(electoral_district))
locs <- select(ppr_gc2, longitude, latitude)
sp_ppr <- SpatialPointsDataFrame(locs, data=ppr_gc2, proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
shp <- readOGR("electoral_divisions_gps.shp")
dublin_counties <- c("Fingal", "Dn Laoghaire-Rathdown", "Dublin City", 
                     "South Dublin")
subset <- shp[as.character(shp@data$COUNTYNAME) %in% 
              dublin_counties, ]
dubcity <- shp[as.character(shp@data$COUNTYNAME)=="Dublin City",]
mapdata <- fortify(subset)

dubcity <- filter(ppr_gc2, geo_county %in% dublin_counties)
dubcity_samp <- sample_frac(dubcity, size=0.3)


#+END_SRC

#+BEGIN_SRC R :exports code :eval no
stan_elec_dub <- stan_lmer(log(price)~region+
                               (postcode|year)+
                               (elec_dist|postcode),
                           data=dubcity_samp2)
#+END_SRC

* Models, yo! :noexport:

- Insprired by regression modelling strategies, I've decided to fit some models to the datasets!
-

#+BEGIN_SRC R :session :results none :exports code :eval no
require(lme4)
prop_size_num <- with(train_sample, gsub("[A-zA-z]+", "", x=property_size_description))
prop2 <- gsub("\\s+", " ", x=prop_size_num)
train_sample$property_size_description <- fct_explicit_na(prop2)
pprlm0 <- lm(price~property_size_description+county, data=train_sample)
pprlm1 <- lm(price~property_size_description+county+year, data=train_sample)
pprlm2 <- lm(price~property_size_description+county+(year)^2, data=train_sample)
pprlm3_log <- lm(log(price)~property_size_description+county+poly(year, 2), data=train_sample)
ppr_lmer <- lmer(price~property_size_description+county+(1|year), data=train_sample)
ppr_lmer2 <- lmer(price~property_size_description+(1|county)+(1|year), data=train_sample)
ppr_lmer3 <- lmer(price~property_size_description+(county|year), data=train_sample)
                      
#+END_SRC

#+BEGIN_SRC R :session :results none :exports none
save_model <- function(x) saveRDS(x, paste(substitute(x), ".rds", sep=""))
#+END_SRC
- So we start with a simple model (actually Harrell suggests starting
  with the full model, but whatevs).
- We look at property size and county

#+BEGIN_SRC R :session :colnames yes :eval no
print(tidy(pprlm0), digits=2)
#+END_SRC

#+RESULTS:
| term                                                                                                           |          estimate |        std.error |            statistic |              p.value |
|----------------------------------------------------------------------------------------------------------------+-------------------+------------------+----------------------+----------------------|
| (Intercept)                                                                                                    |  246592.355319382 | 22309.8458321105 |     11.0530730321974 | 2.39769414793523e-28 |
| property_sizegreater than or equal to 125 sq metres                                                            |  45484.9828972044 | 50792.1385409464 |     0.89551226240526 |    0.370520372629761 |
| property_sizegreater than or equal to 38 sq metres and less than 125 sq metres                                 | -123062.062103286 | 6520.67976383538 |    -18.8725817798637 | 5.40409289837696e-79 |
| property_sizeless than 38 sq metres                                                                            | -139153.744111224 | 11436.4782227443 |    -12.1675345679828 | 5.54844740849382e-34 |
| property_sizenï¿½os mï¿½ nï¿½ nï¿½ cothrom le 38 mï¿½adar cearnach agus nï¿½os lï¿½ nï¿½ 125 mï¿½adar cearnach | -34787.1541619935 | 329082.838527376 |   -0.105709414436996 |     0.91581360491836 |
| property_sizen?os l? n? 38 m?adar cearnach                                                                     | -289268.172060552 | 465176.798332988 |   -0.621845657601962 |    0.534048003414783 |
| property_sizenos m n n cothrom le 38 madar cearnach agus nos l n 125 madar cearnach                            |  265974.993601415 | 465258.065159164 |    0.571671967707696 |    0.567548436816663 |
| countyCavan                                                                                                    | -67386.8428078387 |  27604.285212653 |    -2.44117325584473 |   0.0146452685444054 |
| countyClare                                                                                                    |  3262.58310866583 | 29353.0883591969 |    0.111149568615787 |    0.911498472659221 |
| countyCork                                                                                                     |  54626.0676435452 | 23152.8454969902 |     2.35936734647352 |    0.018312382727438 |
| countyDonegal                                                                                                  | -33214.6667027356 | 27107.1112906174 |      -1.225311924485 |    0.220467168050835 |
| countyDublin                                                                                                   |  157720.816741127 | 22213.0909331943 |     7.10035434579865 | 1.27154981509977e-12 |
| countyGalway                                                                                                   |  7985.69551670781 |  24920.803876231 |    0.320442934199423 |    0.748634775848122 |
| countyKerry                                                                                                    | -8585.19234977866 | 27003.2910808002 |   -0.317931333780381 |    0.750539197932665 |
| countyKildare                                                                                                  |  87432.6510791813 | 23925.4740813581 |     3.65437486345592 | 0.000258241108035178 |
| countyKilkenny                                                                                                 | -5300.85666387262 | 29915.0538970484 |   -0.177196961841197 |    0.859354857255014 |
| countyLaois                                                                                                    | -31344.5755090978 | 28857.1080298167 |    -1.08619947212697 |    0.277399325110503 |
| countyLeitrim                                                                                                  | -63582.0691861116 | 30575.6323540152 |    -2.07950136402533 |   0.0375796156761726 |
| countyLimerick                                                                                                 |  11956.6327620323 |  27281.317581133 |    0.438271821970254 |     0.66119235010888 |
| countyLongford                                                                                                 | -67918.3145925299 | 31931.7482307845 |    -2.12698390647624 |   0.0334294134886982 |
| countyLouth                                                                                                    |  13866.1066051194 | 26756.3724085751 |    0.518235670866782 |    0.604297604523882 |
| countyMayo                                                                                                     | -30149.8314094527 | 28785.7928332954 |    -1.04738582619755 |    0.294929956251686 |
| countyMeath                                                                                                    |  54094.1343880218 | 25094.6993133915 |     2.15560002184027 |   0.0311226990743876 |
| countyMonaghan                                                                                                 | -18903.9355205169 | 32459.1164635881 |   -0.582392177609729 |    0.560306849308809 |
| countyOffaly                                                                                                   | -36812.6708331868 |  35182.541681191 |    -1.04633346751259 |     0.29541538090585 |
| countyRoscommon                                                                                                | -71488.1744394423 | 30172.4625315217 |    -2.36931852561777 |   0.0178270807894992 |
| countySligo                                                                                                    | -5952.51747560606 | 29720.3152512028 |    -0.20028446620751 |    0.841259444367382 |
| countyTipperary                                                                                                |  -98.013865241359 | 29498.3319672576 | -0.00332269178305241 |    0.997348902109121 |
| countyWaterford                                                                                                | -4941.33080099257 | 29682.6663385066 |   -0.166471931619643 |    0.867786672469043 |
| countyWestmeath                                                                                                | -20981.6822689529 | 29174.9622977796 |   -0.719167416732182 |    0.472043276176486 |
| countyWexford                                                                                                  | -16137.6906658686 | 26265.5594532136 |   -0.614404985152299 |    0.538952319961537 |
| countyWicklow                                                                                                  |  98911.0592161821 | 26167.0326019108 |     3.77998761727989 | 0.000157134375405939 |


- The weird thing here is that property size swaps sign, even though one would expect it to be ordered
- Might be worth explictly including the ordering to see if that makes a difference


#+BEGIN_SRC R :session :colnames yes :eval no
tidy(pprlm1)
#+END_SRC

#+RESULTS:
| term                                                                                                           | estimate | std.error | statistic | p.value |
|----------------------------------------------------------------------------------------------------------------+-------+-------+-------+-------|
|                                                                                                                |   <5> |   <5> |   <5> |   <5> |
| (Intercept)                                                                                                    | -13289787.6546072 | 2660321.02838502 | -4.99555787170353 | 5.89893867907463e-07 |
| property_sizegreater than or equal to 125 sq metres                                                            | 24886.5902747126 | 50932.5733036076 | 0.488618356790346 | 0.625115423512882 |
| property_sizegreater than or equal to 38 sq metres and less than 125 sq metres                                 | -125284.7124257 | 6532.64346917915 | -19.1782565536891 | 1.69742793436386e-81 |
| property_sizeless than 38 sq metres                                                                            | -137511.794085593 | 11436.3729581179 | -12.0240739427777 | 3.15687465763404e-33 |
| property_sizenï¿½os mï¿½ nï¿½ nï¿½ cothrom le 38 mï¿½adar cearnach agus nï¿½os lï¿½ nï¿½ 125 mï¿½adar cearnach | -29886.1193334778 | 328950.200900384 | -0.0908530204622925 | 0.927609975911238 |
| property_sizen?os l? n? 38 m?adar cearnach                                                                     | -281455.056206578 | 464989.849639389 | -0.605292903543707 | 0.544988894184133 |
| property_sizenos m n n cothrom le 38 madar cearnach agus nos l n 125 madar cearnach                            | 252866.543182569 | 465075.683033987 | 0.543710523700054 | 0.586644658940625 |
| countyCavan                                                                                                    | -67179.0813420542 | 27593.0711656676 | -2.43463588879665 | 0.0149124698236477 |
| countyClare                                                                                                    | 2004.48410587179 | 29342.1734624332 | 0.0683140977418775 | 0.945536049271583 |
| countyCork                                                                                                     | 51845.5712429473 | 23149.8644716316 | 2.23956262493373 | 0.0251264996356337 |
| countyDonegal                                                                                                  | -32637.3777569476 | 27096.3070617723 | -1.20449542007858 | 0.228407487031249 |
| countyDublin                                                                                                   | 154426.243508611 | 22213.4806364698 | 6.95191564239043 | 3.67596980664369e-12 |
| countyGalway                                                                                                   | 7007.05257171473 | 24911.3951393681 | 0.281279010369086 | 0.778498313919331 |
| countyKerry                                                                                                    | -10025.5700201416 | 26993.77587048 | -0.371403025210171 | 0.710339949532822 |
| countyKildare                                                                                                  | 84882.9123093819 | 23920.9771739931 | 3.54847177404052 | 0.000388054775015898 |
| countyKilkenny                                                                                                 | -5949.72791378275 | 29903.1402761848 | -0.198966658980669 | 0.842290150130267 |
| countyLaois                                                                                                    | -29729.3147236547 | 28847.1000740413 | -1.03058243800413 | 0.302744837950052 |
| countyLeitrim                                                                                                  | -65334.657286398 | 30565.1184197908 | -2.13755616415652 | 0.0325607008819415 |
| countyLimerick                                                                                                 | 11801.898421057 | 27270.2218367664 | 0.432776032835396 | 0.66518054330711 |
| countyLongford                                                                                                 | -70986.3693287874 | 31924.4355886257 | -2.22357476396792 | 0.0261843560867568 |
| countyLouth                                                                                                    | 12740.1794016588 | 26746.3888434733 | 0.476332692096028 | 0.633840786425013 |
| countyMayo                                                                                                     | -32753.1952195762 | 28778.6154750361 | -1.13810878942345 | 0.255083987993329 |
| countyMeath                                                                                                    | 52751.8610281248 | 25085.8642766795 | 2.10285204632811 | 0.0354868675500598 |
| countyMonaghan                                                                                                 | -17170.224205544 | 32447.6835467957 | -0.529166409700134 | 0.596693862765596 |
| countyOffaly                                                                                                   | -35038.6398156055 | 35169.9385992789 | -0.996266732644336 | 0.319128457753417 |
| countyRoscommon                                                                                                | -71518.9929438411 | 30160.172769431 | -2.37130581083174 | 0.0177315259416125 |
| countySligo                                                                                                    | -6193.37074175532 | 29708.2467652218 | -0.208473114913184 | 0.834860963151013 |
| countyTipperary                                                                                                | 1013.39747360199 | 29487.1251530274 | 0.0343674559097513 | 0.972584359193861 |
| countyWaterford                                                                                                | -5043.75421443332 | 29670.5823082727 | -0.16999175014597 | 0.865017750826684 |
| countyWestmeath                                                                                                | -17665.9106915961 | 29170.3574550343 | -0.605611731663827 | 0.54477711133572 |
| countyWexford                                                                                                  | -17301.3594234028 | 26255.8564736729 | -0.658952391850219 | 0.509931309650836 |
| countyWicklow                                                                                                  | 97691.5071862813 | 26157.4718286658 | 3.73474576695221 | 0.000188246389299767 |
| year                                                                                                           | 6725.57727003072 | 1321.73934030496 | 5.08842936344693 | 3.63178832886814e-07 |
#+TBLFM:



* Article :noexport:


** DONE Clean up XL file before import :noexport:
- nice, mindless work for fifteen minutes or so. 
- only took 10 mins, woo!
Mortgage approval and drawdown data can be found [[https://www.bpfi.ie/publications/bpfi-mortgage-approvals-report/][here]].

Need to nose around their site to find more.

Have put an XL file with terrible formatting into this folder

** sratchwork :noexport:
 #+BEGIN_SRC R :session :results output graphics :file county1.png :exports results
ggplot(prop_df, aes(x=date_of_sale, y=price))+facet_wrap(~county)+geom_line()
 #+END_SRC

 #+RESULTS:
 [[file:county1.png]]
 - Nothing except Dublin matters. 

 #+BEGIN_SRC R :session :results output graphics :file county2.png :exports results
ggplot(prop_df, aes(x=date_of_sale, y=log(price, 10)))+facet_wrap(~county)+geom_line()
 #+END_SRC

 #+RESULTS:
 [[file:county2.png]]

 - the log brings out some kinda trend

 #+BEGIN_SRC R :session :results output graphics :file county3.png :exports results
ggplot(prop_df, aes(x=date_of_sale, y=log(price, 10)))+facet_wrap(~county)+geom_line()+geom_smooth(method="lm")
 #+END_SRC

 #+RESULTS:
 [[file:county3.png]]

 - but it's super noisy
 - we have whole developments being sold also
 - they should probably be removed from the data

 #+BEGIN_SRC R :session :results output graphics :file dublin1.png :exports results
filter(prop_df, county=="Dublin") %>% ggplot(aes(x=date_of_sale, y=price))+geom_line()+facet_wrap(~postal_code)
 #+END_SRC

 #+RESULTS:
 [[file:dublin1.png]]
 #+BEGIN_SRC R :session :results output graphics :file dublin2.png :exports results
filter(prop_df, county=="Dublin", postal_code=="Dublin 7") %>% ggplot(aes(x=date_of_sale, y=price))+geom_line()+facet_wrap(~postal_code)+scale_y_continuous(labels=scales::dollar)
 #+END_SRC

 #+RESULTS:
 [[file:dublin2.png]]

 #+BEGIN_SRC R :session :results output graphics :file dublin3.png :exports results
filter(prop_df, county=="Dublin", postal_code=="Dublin 7", is.na(property_size_description)) %>% ggplot(aes(x=date_of_sale, y=price))+geom_line()+facet_wrap(~postal_code)+scale_y_continuous(labels=scales::dollar)
 #+END_SRC

 #+RESULTS:
 [[file:dublin3.png]]
 #+BEGIN_SRC R :session :results output graphics :file dublin5.png :exports results
filter(prop_df, county=="Dublin", postal_code=="Dublin 7", is.na(property_size_description), price<2e6) %>% ggplot(aes(x=date_of_sale, y=price))+geom_line()+facet_wrap(~postal_code)+scale_y_continuous(labels=scales::dollar)
 #+END_SRC

 #+RESULTS:
 [[file:dublin5.png]]
 #+BEGIN_SRC R :session :results output graphics :file dublin6.png :exports results
mutate(prop_df, month=month(date_of_sale, label=TRUE), year=year(date_of_sale)) %>% filter( county=="Dublin", postal_code=="Dublin 7", is.na(property_size_description), price<2e6) %>% ggplot(aes(x=date_of_, y=price))+geom_point()+facet_wrap(~month)+scale_y_continuous(labels=scales::dollar)
 #+END_SRC

 #+RESULTS:
 [[file:dublin6.png]]



* First Article
#+BEGIN_QUOTE
It is a truth universally acknowledged that a married man in
possession of a good fortune must be in want of a house.
#+END_QUOTE

I've taken the liberty of updating good old Jane's statement for more
contemporary times. This article talks about my process of looking at
and understanding property prices in Ireland.

Some background for my non-Irish readers:
- Ireland was mostly poor from the 1930's to the 1990's
- Banks were very hesitant to lend money to anyone
- And the interest rates were very high

This all changed during the Celtic Tiger period, when a flood of cheap
euro-denominated money was dropped in by German and french
helicopters [fn:1]. This lead (predictably) to a massive property
bubble, which was sadly interrupted by the advent of financial
contagion in 2007. Over the next few years, the economy fell apart, we
were dragged into a bailout program, and house prices collapsed.

A few years after that, they started to recover, and right now, we
have a massive housing crisis in that people can't afford houses,
which puts more stress on the rental market, increasing prices.

Given that I don't trust the property-porn engaged in by the media and
government, I decided to take matters into my own hands, and consult
the property price register (PPR for future reference) and figure out
whether or not I should buy or continue to slum it in a hell of
short-term lets and fear [fn:2].

** Property Price Register

So, the property price register was established in the depths of the
economic crisis [fn:3], and so has a number of problems. The most
obvious is that we only have data from 2010. Given data from the boom,
it would be a much easier task to estimate tops and bottoms, but we
can only work with what we have. The second problem with the data is
that Ireland doesn't have postcodes [fn:4], which makes localisation
of the properties a challenge.

Nonetheless, the nice people at the [[https://www.propertypriceregister.ie/website/npsra/pprweb.nsf/page/ppr-home-en][PPR]] have collated the data for us,
so there is that. They helpfully provide the data in CSV format, and
have a document which contains all of the property data, which can be
found [[https://propertypriceregister.ie/website/npsra/pprweb.nsf/PPRDownloads?OpenForm][here]] [fn:5]


So, on with the show! [fn:6]

*** Data Cleaning

In data analysis, much as in life, cleaning is important.
Unfortunately, while keeping ones house and body cleaning rarely takes
up all of one's time, data cleaning, can,  and often does take up almost all
of the time. So, enjoying (or at least not hating) this process,
is an important career skill.

Anyways, so the first thing we do is look at the data

#+BEGIN_SRC R :session *R* :exports both
dat <- readxl::read_excel("PPR-ALL.xlsx", sheet="PPR-ALL")
names(dat)
#+END_SRC

#+RESULTS:
| Date of Sale (dd/mm/yyyy) |
| Address                   |
| Postal Code               |
| County                    |
| Price (�)                 |
| Not Full Market Price     |
| VAT Exclusive             |
| Description of Property   |
| Property Size Description |

OK, so we have date (nicely, they have specified the format),
address, postal code, county.

Note that price has a weirdass character
in there. This is probably because I don't really understand how
encodings work [fn:21].

I have some problems with these names. The spaces will cause problems,
and capitalisation is so passe. I'll fix these problems with some
code.

#+BEGIN_SRC R :session :exports code
normalise_names <- function(df) {
    nms <- names(df)
    normed <- iconv(
        tolower(
            gsub("([[:space:]]|[[:punct:]])+", "_",
                 x=nms)),
        "latin1", "ASCII", sub="")
    drop_usc <- gsub("([a-z_])_*$", "\\1",
                     x=normed)
    names(df) <- drop_usc
    df
}
#+END_SRC

#+RESULTS:


This function does a few things:
1) it replaces spaces or punctuation with underscores[fn:22]
2) it then converts all text to lowercase 
3) Then we exert cultural imperialism and export to ascii[fn:24]
4) finally, we replace multiple underscores with one (for tidyness, mostly)

#+BEGIN_SRC R :session :exports both 
dat2 <- normalise_names(dat)
names(dat2)
#+END_SRC

#+RESULTS:
| date_of_sale_dd_mm_yyyy   |
| address                   |
| postal_code               |
| county                    |
| price                     |
| not_full_market_price     |
| vat_exclusive             |
| description_of_property   |
| property_size_description |

Much better. Little things like this add up to a better, more
productive life.

Next, we need to remove the Euro symbols from the input doc (which
have become hopelessly mangled). Again, we abuse iconv, this time on
a particular column. My life would be much easier if I took the time
to learn how to handle encodings [fn:7]

#+BEGIN_SRC R :exports code :results none
fix_price <- function(x) {
    nopunct <- gsub(",|\\.", "", x=x)
    nums <- iconv(nopunct, "latin1", "ASCII", sub="")
}
dat2$price <- with(dat2, fix_price(price))
#+END_SRC




- For some bizzare reason, price is in cent. We handle that here to avoid wasting time. 
- These preliminaries accomplished, we can move towards actually looking at the data

#+BEGIN_SRC R :session :exports code :results none

prop_df <- dat2 %>%
    mutate(date_of_sale=lubridate::dmy(date_of_sale_dd_mm_yyyy),
           not_full_market_price=as.factor(not_full_market_price),
           postal_code=as.factor(fct_explicit_na(postal_code)),
           vat_exclusive=as.factor(vat_exclusive),
           county=as.factor(county),
           price=as.numeric(price)/100)
prop_df2 <- mutate(prop_df,
                   year=lubridate::year(date_of_sale),
                   month=lubridate::month(date_of_sale),
                   day=lubridate::day(date_of_sale))

prop_sample <- sample_frac(prop_df2, size=0.1)
#+END_SRC

- We also take a ten percent sample, just to speed up the analysis [fn:8]

#+RESULTS:

*** Wasting Time with Plots

#+BEGIN_SRC R :session :results output graphics :file pricedens.png :exports results
ggplot(prop_df, aes(x=price))+geom_density()
#+END_SRC

#+RESULTS:
[[file:pricedens.png]]

- So, when we do the simplest plot ever (a one-way summary of price),
  it looks pretty weird.
- Most of the density is in the one line near zero, and there's a
  long tail that goes up to 100mn
- who are these people buying houses for many millions of euros?
- most importantly, how can I become one of them? 

The first thing to do when you find something weird like this is to
take a look at the observations and see if they are different [fn:9].
Let's sort the data by price and look at the addresses.

#+BEGIN_SRC R :session :colnames yes :exports both
arrange(prop_df, desc(price)) %>%
    select(address, county, date_of_sale, price) %>%
    head()
#+END_SRC

#+RESULTS:
| address                                       | county | date_of_sale |     price |
|-----------------------------------------------+--------+--------------+-----------|
| 2 CANNON HOUSE, CLANCY QUAY, DUBLIN 8         | Dublin |   2018-06-22 | 139165000 |
| APT 204 THE ALLIANCE, THE GASWORKS, BARROW ST | Dublin |   2018-06-22 |  87928183 |
| Block F  K and L Central Park, Leopardstown   | Dublin |   2014-07-24 |  86365000 |
| Blocks F  K and L Central Park, Leopardstown  | Dublin |   2014-03-28 |  70503358 |
| 182 THE ELYSIAN, EGLINTON STREET, CORK        | Cork   |   2018-07-27 |  69873482 |
| Binary Hub  Roe's Lane, Bonham Street         | Dublin |   2016-12-19 |  69208163 |

OK, so the Cannon house thing must be more than one apartment. Otherwise things have gone even
madder than I thought. Also Apt 204 the Alliance, and 182 the Elysian [fn:10]. Entire 
apartment blocks for 76-86 million makes more sense, however.

#+BEGIN_SRC R :session :colnames yes :exports both
arrange(prop_df, desc(price)) %>%
    select(address, county, date_of_sale, price) %>%
    tail()
#+END_SRC

#+RESULTS:
| address                                  | county    | date_of_sale | price |
|------------------------------------------+-----------+--------------+-------|
| 7 Greenfields, Lanesboro Road, Roscommon | Roscommon |   2015-05-22 |  5179 |
| 8 Greenfields, Lanesboro Road, Roscommon | Roscommon |   2015-05-22 |  5179 |
| 9 Greenfields, Lanesboro Road, Roscommon | Roscommon |   2015-05-22 |  5179 |
| LAVALLY, BALLINTOGHER                    | Sligo     |   2014-05-01 |  5177 |
| Loghnabradden, Fintown, Co. Donegal      | Donegal   |   2012-01-11 |  5079 |
| CLOGHAN, GLENCOLMCILLE, DONEGAL          | Donegal   |   2014-07-18 |  5079 |


- there must be something really wrong with these houses.

It does tell us something about the tails[fn:11] of the crash, in that
property was worth so little in Roscommon in 2015 that this happened.
It may also be an error, as I don't know what kind of checking goes into 
this data. 

#+BEGIN_SRC R :session :results output graphics :file countyearly.png :exports results
year2 <- prop_df2 %>% group_by(year) %>% summarise(count=n())
ggplot(year2, aes(x=year, y=count))+geom_line()
#+END_SRC

#+RESULTS:
[[file:countyearly.png]]
- Note that 2018 hasn't finished yet, which distorts the trend.

However, it's interesting to note that there weren't that many sales.
Even at 55k it's only 5% of the housing stock [fn:12]. That's not a huge amount,
but given the lack of data for the boom, it's very difficult to make those judgements. 

#+BEGIN_SRC R :session :results output graphics :file yearly.png :exports results
countyear <- prop_df2 %>%
    filter(price<2e6)  %>%
    group_by(year, county) %>%
    summarise(count=n(),
              med=median(price),
              mean=mean(price))
ggplot(countyear, aes(x=year, y=med, colour=county))+
    geom_line()+
    scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+END_SRC

#+RESULTS:
[[file:yearly.png]]

[[file:yearly.png]]

So the overall trend is pretty clear. The chart above shows median prices.
For those of you who aren't stats nerds, the median is the middle value.
If you laid out all the prices in a line, the median would be the middle of 
the line. This means that it's a reasonable summary of the distribution. 

Really, you should look at the distribution itself, but aggregations
such as the median are useful for plots and telling people about. 


#+BEGIN_SRC R :session :results output graphics :file meanyearly.png :exports results
ggplot(countyear, aes(x=year, y=mean, colour=county))+geom_line()+scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+END_SRC

#+RESULTS:
[[file:meanyearly.png]]
- The mean, which is the sum divided by the length of the set of numbers is much higher.
This is pretty normal in most sets of data because huge values at the top or bottom
have more impact on the mean. Again, the same trend is pretty visible.

Note that I've filtered out the properties sold for more than 2mn, as these are mostly apartment
blocks and groups of houses. These probably shouldn't be looked at for this analysis [fn:13]. 

Looking closer at Dublin (and breaking down by postal code), we can see some more interesting patterns. 

#+BEGIN_SRC R :session  :results output graphics :file dubmonthly.png :exports results
filter(prop_df, county=="Dublin", price<1e6, 
county==str_match(county, "^Dublin")) %>%
    ggplot( aes(x=date_of_sale, y=price))+
    geom_point()+
    facet_wrap(~postal_code)+
    geom_smooth()
#+END_SRC

#+RESULTS:
[[file:dubmonthly.png]]
- Prices are definitely trending up, but there are some large outliers
  across the board.

Like this former house owned by an executive in [[https://www.irishtimes.com/life-and-style/homes-and-property/new-to-market/where-the-gardens-sweep-down-to-the-liffey-1.2650733][Dublin 20, for
example]]. That place definitely doesn't reflect the average experience
in that area. In general, I'm beginning to think that there are
multiple housing markets in Ireland, not one. This makes sense as one
can only compete against others in the same price range. This would
suggest that I should regard the house buying denizens of South Dublin
to be some kind of alien species. I do enjoy when a data-driven theory
agrees with my priors.


#+BEGIN_SRC R  :session  :results output graphics :file dubsalesdaily.png :exports results
avbyday <- filter(prop_df, price<2e6, county=="Dublin") %>% group_by(date_of_sale, postal_code) %>% summarise(med_price=median(price), sales=n(), min_price=min(price), max_price=max(price), var_price=var(price), mean_price=mean(price))
ggplot(avbyday, aes(x=date_of_sale, y=sales))+geom_line()+facet_wrap(~postal_code)
#+END_SRC

#+RESULTS:
[[file:dubsalesdaily.png]]
- Long periods of not much activity, followed by sudden spikes.
- We should probably dig into some of the spikes to figure out what
  they mean.
- It's possible that they may be one overall transaction recorded as
  multiples (for instance, when an entire apartment block is sold to
  one developer, and it's recorded unit by unit).


** Mortgage Data

Most people who buy houses need mortgages, unfortunately. Fortuntately
for me, we have a (reasonably good) source of information in this on
the <INSERT_ORG_HERE> website.

The data goes back to 2011 (so not quite as much as our house price
data), and is collected at a monthly cadence. It appears to be most of
the high-street banks in ireland, and is only for mortgages on residential
properties.

It comes in an Excel sheet. Whoever released this seemed to value the
aesthetic principles of the dataset in preference to usability in
computer programs. Given that the primary audience seems to be
journalists, this probably makes sense.

I did some artisanal data cleaning [fn:14] on the file, and imported
all the data as per below.

#+BEGIN_SRC R :session :results none :exports code
require(readxl)
mortgage_sheets <- excel_sheets("Website-mortgage-approvals-time-series-Sep2018.xls")
volumes <- read_excel("Website-mortgage-approvals-time-series-Sep2018.xls",
                      sheet="Volumes", skip=4)
names(volumes)[c(1:2, 8)] <- c("year", "month", "total")
volumes2 <- normalise_names(volumes)

values <- read_excel("Website-mortgage-approvals-time-series-Sep2018.xls",
                     sheet="Values", skip=4)
names(values)[c(1:2, 8)] <- c("year", "month", "total")
values2 <- normalise_names(values)

av_values <- read_excel("Website-mortgage-approvals-time-series-Sep2018.xls",
                        sheet="Average Value", skip=4)
names(av_values)[c(1:2, 8)] <- c("year", "month", "total")
av_values2 <- normalise_names(av_values)
valtidy <- gather(values2, key="type", value="value", 3:7) %>%
    mutate(date=lubridate::ymd(paste(
                               year, month, "01",
                               sep="-_")))
voltidy <- gather(volumes, key="type", value="volume", 3:7) %>%
    mutate(date=lubridate::ymd(paste(
                               year, month, "01",
                               sep="-_")))
av_valtidy <- gather(av_values2, key="type", value="av_value", 3:7) %>%
    mutate(date=lubridate::ymd(paste(
                               year, month, "01",
                               sep="-_")))
#+END_SRC


Note the repeated code. Sometimes, a little copying is better than a
large dependency. Sometimes people are lazy. Who can ever tell, in
this workaday world?

The volumes tab is count, the values tab is in millions (1e6), and the
average values tab is (presumably) the first divided by the second.
There's also a notes tab, which we don't import [fn:15]

So, below we can see the volumes of mortgage approvals since 2011. 

#+BEGIN_SRC R :session  :results output graphics :file mortgagecount.png :exports results
ggplot(voltidy,
       aes(x=date, y=volume,
           colour=type))+
    geom_line()+
    geom_smooth()
#+END_SRC

#+RESULTS:
[[file:mortgagecount.png]] [[file:mortgagecount.png]] The most striking thing
about this graph is the complete absence of first-time buyers between
2011 and 2014. I am tempted to declare it a data mistake, but given
the economic conditions it is a little plausible. They may also
suppress small numbers in the interests of privacy. It would be good
if I figured this out, but I'll leave it for now.

When they do come back, they come back all at once, with near level
FTB and traders-up. If this pattern held for the entirity of the boom,
no wonder that things have gotten so messed up since.

Next, we look at the total value of mortgages issued to particular
kinds of buyers over the time period.

#+BEGIN_SRC R :session  :results output graphics :file mortgagevols.png :exports results
ggplot(valtidy, aes(x=dates, y=count*1.e6, colour=type))+
    geom_line()+
    geom_smooth()+
    ylab("total issue value ")+ 
    scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+END_SRC

#+RESULTS:
[[file:mortgagevals.png]]


- So we have been 1-2bn per year for each of the FTB and Mover Purchase categories.

We can see what the average looks like in the plot below. 


#+BEGIN_SRC R :session  :results output graphics :file mortgage_av_vals.png :exports results
ggplot(av_valtidy, aes(x=date, y=av_value, colour=type))+
    geom_line()+
    geom_smooth()+
    ylab("average issue value ")+
    scale_y_continuous(labels=scales::dollar_format(prefix="€" ))
#+END_SRC

#+RESULTS:
[[file:mortgage_av_vals.png]]


This chart is actually more interesting than either of the previous
two. We can see that the average value given to FTB is significantly
less than that for Mover Purchase. In some sense, this is weird as one
would expect the movers to need less of a mortgage, given the existence 
of their previous property.

Another point that is obvious from the plot is the re-appearence of
remortgaging in terms of value in 2014. Perhaps not coincidentally,
the state emerged from the bailout program in December [[https://www.irishtimes.com/business/economy/jean-claude-trichet-and-the-irish-bailout-a-timeline-1.1990882][2013 (paywall,
but they have a few free articles)]]. 

Did the recovery in property prices encourage people to re-mortgage?
Did the banks encourage these (presumably wealthy) borrowers?
it's difficult to estbalish this from the data.

Additionally, given that I want to buy a house in Dublin, I'm not sure
how useful the overall figures are. Unfortunately, this is all we have,
which means we probably need more data (don't worry, we'll come back
to the mortgage data eventually [fn:16]).

Next stop, the CSO!

** Residential Prices by Month, Type of Property (HPM06)

The CSO have this arcane windows only application that people 
are supposed to use to get data. Being on Linux, I was SOL until
some kind soul wrote code to access their database through an API.

They use a weird-ass format called JSON-Stat, which I know little
about, but fortunately I can just call a function and get the data
back as a dataframe.

#+BEGIN_SRC R :session  :exports both :results none
source("rcso/read_api.R")
prices <- getJSONstat("HPM06")

#+END_SRC

#+BEGIN_SRC R :session :colnames yes :exports both
head(prices)
#+END_SRC

#+RESULTS:
| type_of_residential_property          |      month | statistic                                                                 | value |
|---------------------------------------+------------+---------------------------------------------------------------------------+-------|
| National - all residential properties | 2005-01-01 | Residential Property Price Index (Base Jan 2005 = 100)                    | 100   |
| National - all residential properties | 2005-01-01 | Percentage Change over 1 month for Residential Property Price Index (%)   | nil   |
| National - all residential properties | 2005-01-01 | Percentage Change over 3 months for Residential Property Price Index (%)  | nil   |
| National - all residential properties | 2005-01-01 | Percentage Change over 12 months for Residential Property Price Index (%) | nil   |
| National - all residential properties | 2005-02-01 | Residential Property Price Index (Base Jan 2005 = 100)                    | 100.8 |
| National - all residential properties | 2005-02-01 | Percentage Change over 1 month for Residential Property Price Index (%)   | 0.8   |

- So the type variable tells us what we are measuring, the statistic defines what the value means.
- This is awkward data to work with, but tidying it up should help.
We'll keep type in the rows, but move all the statistics into their own columns, to facilitate plotting.

#+BEGIN_SRC R :session :results none :exports code
prices2 <- spread(prices, key="statistic", value="value")
prices3 <- normalise_names(prices2)
#+END_SRC

The names are still really long, but at least they don't have spaces anymore. 

The variable we care about here is the index. This is normalised to 2005,
at 100, so we can interpret the graph as the change since then. 

#+BEGIN_SRC R :session :results output graphics :file prices200518.png :exports results
filter(prices3, type_of_residential_property=="National - all residential properties") %>%
    ggplot(aes(x=lubridate::ymd(month), y=residential_property_price_index_base_jan_2005_100_))+geom_line()
#+END_SRC

#+RESULTS:
[[file:prices200518.png]] 

Wow, so overall we're in 2005 again. I remember
telling people not to buy houses around then. 
Mind you I was just bitter because no-one would give me 
a mortgage [fn:17]. 

This kinda scares the hell out of me, but I suspect that
the numbers of houses being sold today are a fraction of those
sold during the boom. 

*** TODO Figure out how to get some data on house prices sales during the boom. :noexport:


Next, we can break this down by county/region. 

#+BEGIN_SRC R :session :results output graphics :file prices_county200518.png :exports results
filter(prices3, type_of_residential_property!="National - all residential properties") %>%
    ggplot(aes(x=lubridate::ymd(month), y=residential_property_price_index_base_jan_2005_100_))+geom_line()+facet_wrap(~type_of_residential_property)+geom_hline(yintercept=100, colour="red")
#+END_SRC

#+RESULTS:
[[file:prices_county200518.png]] This plot is really informative on my
hi-res screen, but is unlikely to prove useful 
to any of my readers, so I'll break it down into 
number of plots.

#+BEGIN_SRC  R :session :results output graphics :file dub_areas.png :exports results
national <- c("National - all residential properties",
              "National - apartments",
              "National - houses")
nondub_national <- c("National excluding Dublin - all residential properties",
                     "National excluding Dublin - apartments",
                     "National excluding Dublin - houses")
dublin_ov <- c("Dublin - all residential properties",
                  "Dublin - apartments",
               "Dublin - houses")
dub_areas <- c("Dublin City - houses",
               "Dún Laoghaire-Rathdown - houses",
               "Fingal - houses",
               "South Dublin - houses")
filter(prices3, type_of_residential_property %in% dub_areas) %>%
    ggplot(aes(x=lubridate::ymd(month), y=residential_property_price_index_base_jan_2005_100_))+geom_line()+facet_wrap(~type_of_residential_property)+geom_hline(yintercept=100, colour="red")
#+END_SRC


One of the big problems with most of the datasets available is that they start during the crash.
This makes it very difficult to accurately estimate any trend. However,
we can get new property prices by year and major area from the department of housing. 
While this isn't perfect, it's better than nothing.

** New Property Prices 

#+BEGIN_SRC R :session :results none :exports none
newprices <- read_csv("yearly_property_prices.csv", skip=1, na="n/a")
newprices2 <- normalise_names(newprices)
newprices3 <- gather(newprices2, key="area", value="price", 2:8)

#+END_SRC


#+BEGIN_SRC R :session :results output graphics :file yearlyprices.png :exports results
ggplot(newprices3, aes(x=year, y=as.numeric(price), colour=area))+
    geom_line()+
    scale_y_continuous(labels=scales::dollar_format(prefix="€" ))+
    facet_wrap(~area)+ylab("price")
#+END_SRC

#+RESULTS:
[[file:yearlyprices.png]]


Wow, this is mental. House prices have basically only gone down a lot once,
and that was the boom. This may be why many Irish people regard property as 
such a good asset. Additionally, we're now seeing ZIRP and QE inject large sums
of money into the economy, which has pumped up asset prices [fn:18]. 

On the other hand, Dublin looked pretty mental in 2015 (when this datasource stops).
Note that this is only for new properties, which is skewed by the scarcity.
I probably need to get some counts to normalise.

Additionally, I need to look at population flows over the time periods concerned.
If I had counts for houses built and available, that would be super useful.
Not sure if such a source exists, however. 

** Pulling it All Together

- So what have we learned from today's excursions? 

Firstly, we reviewed the change in property prices by county and date of sale
from the PPR. This source is entirely comprehensive, but only covers sales,
so we can't figure out demand from this, except by looking at price.
Additionally, the address data is pretty raw, and it would take a whole 
lot of time or cost a whole bunch of money [fn:19] to make it more usable.

If that could be done, the CSO have a lot of good area data which I barely
scratch the surface of here. The most obvious next avenue of approach is
through population statistics and change rates, and this would be expected
to impact demand for housing. 

You could then normalise the data by the population and get more useful 
estimates. 

However, another issue which we're ingoring is rents, as a lot of property 
is not available to be sold. This is becoming increasingly common, especially
in Dublin. I believe that a bunch of the really large transactions in the PPR
are the result of such arrangements. We can get rents from various websites, 
in compliance with all legal processes :) 

Finally, we need to measure the delta between asking and sale prices. Again,
this can be obtained through various web-scraping means and sources. This
delta will then allow us to make somewhat (more) useful decisions with respect to 
property [fn:20]

So, that's what I'm going to do, I guess. To be fair, this post is probably
more like 2-3 articles, but I can't handle not being finished anymore, so 
I'm calling it done. 


*** CountyLists :noexport:

Counties in the province of Connaught
| Name                                                           | 	Irish Equivalent | 	County Town     | 	Population   | 	Area |   |   |   |   |
| Galway                                                         | 	Gaillimh         | 	Galway 	 | 188,598 	6148 |              |   |   |   |   |
| Leitrim 	Liatroim 	Carrick-on-Shannon 	25 | 032 	1588         |                         |                      |              |   |   |   |   |
| Mayo 	Maigh Eo 	Castlebar 	111            | 395 	5585         |                         |                      |              |   |   |   |   |
| Roscommon 	Ros Comán 	Roscommon 	51       | 881 	2547         |                         |                      |              |   |   |   |   |
| Sligo 	Sligeach 	Sligo 	55                | 645 	1836         |                         |                      |              |   |   |   |   |

#+BEGIN_SRC R :session :results none :exports none
provinces <- c("Connaught", "Leinster", "Munster", "Ulster")
connuaght <- c("Galway", "Leitrim", "Mayo", "Roscommon", "Sligo")
leinster <- c("Dublin", "Carlow", "Kildare",
              "Kilkenny", "Laois", "Longford",
              "Louth", "Meath", "Offaly",
              "Westmeath", "Wicklow", "Wexford" )
munster <- c("Clare", "Cork", "Kerry",
             "Limerick", "Tiperrary", "Waterford")
ulster <- c("Cavan", "Donegal", "Monaghan")
#+END_SRC



#+BEGIN_SRC R :session :results none :exports none
discounted_cash_flow <- function(FV=100, r=0.02, t=1) {
    res <- FV/(1+r)^t
    
}

cashflows_by_interest_rate <- function(begin=0, end=10) {
    myseq <- seq(begin, end, by=1)
    mydisc <- discounted_cash_flow(r=myseq)
}
    
#+END_SRC

* Footnotes

[fn:24] this whole lack of understanding of encodings is deeply embarassing

[fn:23] clearly underscores

[fn:22] You could use any character, but I like underscores. It's
totally pointless though, so really we should just standardise on one [fn:23]

[fn:21] definitely. 

[fn:20] though presumably even if I built a working method, it would stop working as more people did it

[fn:19] I checked with a reasonably cheap provider, and it would be 800 quid for the whole sample

[fn:18] I am still not sure if this was just accidental, or malicious

[fn:17] we all partied, dontcha know? 

[fn:16] I don't think anyone except me worries about this

[fn:15] but i did in fact read it. Reading any notes provided is a super-power. 

[fn:14] i.e. by hand

[fn:13] yes, this is really subjective, like almost everything in data
analysis. The advantage here is that I provide code so people can
change that however they want

[fn:12] assuming 1mn households. I should probably check this with the
CSO

[fn:11] geddit?

[fn:10] that dude finally made his money back, after opening really
expensive apartments just as the crash hit

[fn:9] or you could just log it and get one with building a CNN I
guess

[fn:8] turns out sampling is pretty cool, who knew? 

[fn:7] look at how well I'm resisting distraction!

[fn:6] for very nerdy, boring values of show

[fn:5] they also provide commercial data (which I just noticed), but I am resisting the urge to become distracted

[fn:4] we do now, but they are not on any of the addresses

[fn:3] by the Greens actually, so at least I don't feel my vote for them was *entirely* wasted

[fn:2] actually I've been in the same house for many years, but I likes a good simile. 

[fn:1] In fact, it was done via banks, but it doesn't really have the same ring to it now, does it? 

[fn:25] In fact, I already did this, but compleetely forgot to write anything down

* Bayesian models, yo!

- Step one: take a ten percent sample
- Step two: wait loads of time for model to fit, give up in disgust
- Step 3: take a smaller sample
- Step 4: actually look at some data, huzzah!

#+BEGIN_SRC R :results none :exports code
smaller_sample <- sample_frac(prop_sample, size=.1)
#+END_SRC


#+BEGIN_SRC R :exports code :results none :eval no

require(rstanarm)
stanlm <- stan_lmer(log(price, base=10)~year+(1|county), data=smaller_sample)
stanlm2 <- stan_lmer(log(price)~(month|year)+(1|county),
                     data=smaller_sample)
stanlm3 <- stan_lmer(log(price)~+postal_code+
                         (month|year)+(1|county),
                     data=smaller_sample)
stanlm4 <- stan_lmer(log(price)~+vat_exclusive+not_full_market_price+
                         (postal_code|year)+(1|county), data=smaller_sample)
dub_sample <- filter(prop_df2, county=="Dublin") %>% sample_frac(size=0.1)

stanlm_dub <- stan_lmer(log(price)~+vat_exclusive+not_full_market_price+(postal_code|year), data=dub_sample)
#+END_SRC
#+BEGIN_SRC R :results none :exports none :eval no
require(rstanarm)
stanlm <- readRDS("stanlm.rds")
stanlm2 <- readRDS("stanlm2.rds")
stanlm3 <- readRDS("stanlm3.rds")
stanlm_dub <- readRDS("stanlm_dub.rds")
#+END_SRC
So, using a smaller sample, and using a log-transform and normal
approximation for price, we're in business. 

The first model just fits a trend across year with county level intercepts.
In R's formula notation it looks like the below

#+BEGIN_SRC R :exports code :eval no
stanlm <- stan_lmer(log(price)~year+(1|county), data=smaller_sample)
#+END_SRC

- We need lmer because of the grouping factors
- I tried base 10 with this one, to make it easier to convert the
  coefficients in my head

We can use the marvellous *broom* package to grab some details out of the model.
Before that though, we can call the eponymous plot() generic. 

#+BEGIN_SRC R :results output graphics :exports results :file stanlm1pl1.png
plot(stanlm)
#+END_SRC

#+RESULTS:
[[file:stanlm1pl1.png]]

- This model kinda sucks. 
- The intercept dominates, and there's little to nothing of interest. 

#+BEGIN_SRC R :results output graphics :exports results :file stanlm1pl1.png :width 1200 :height 1200
plot(stanlm, plotfun="hist")
#+END_SRC

#+RESULTS:
[[file:stanlm1pl1.png]]

#+BEGIN_SRC R :results output graphics :exports results :file hist1.png :width 1200 :height 800
plot(stanlm, plotfun="hist")+theme(axis.text.x=element_text(angle=-45))
#+END_SRC

#+RESULTS:
[[file:hist1.png]]

This is a histogram of the estimates.

I should probably break these out into a number of plots.
On my hi-res laptop screen they look cool, though. 

We can extract the fitted values and the residuals using the augment function from broom. 

#+BEGIN_SRC R :exports code :results table :colnames yes :eval yes
require(broom)
auglm <- augment(stanlm)

names(auglm)[1] <- "log_price"
head(auglm)
#+END_SRC

#+RESULTS:
|        log_price | year | county   |          .fitted |              .resid |
|------------------+------+----------+------------------+---------------------|
| 5.53781909507327 | 2016 | Dublin   | 5.47106602643247 |  0.0667530686408062 |
| 5.53147891704226 | 2018 | Monaghan | 5.08032996790599 |   0.451148949136268 |
| 5.17609125905568 | 2016 | Clare    |  5.0677609928034 |   0.108330266252281 |
| 5.24303804868629 | 2012 | Cork     | 5.16666133154848 |  0.0763767171378111 |
| 5.34294352175839 | 2010 | Dublin   | 5.36793792528243 | -0.0249944035240421 |
| 4.77815125038364 | 2010 | Limerick | 5.00359292005301 |  -0.225441669669364 |

so we have the fitted values, the original target variable, our covariates and the residuals
Truly we are priveleged to be alive in such a time. To be fair, broom is very nice. 

#+BEGIN_SRC R :results output graphics :exports results :file fitresid1.png
ggplot(auglm, aes(x=.fitted, y=.resid))+geom_point()+geom_smooth(method="lm")
#+END_SRC

#+RESULTS:
[[file:fitresid1.png]]
The residuals increase as we hit the tails, definitely.
In general, we're not accounting for the trend across time here. 
 

#+BEGIN_SRC R :results output graphics :exports results :file fitresid1.png
ggplot(auglm, aes(x=.fitted, y=log_price))+geom_point()+geom_smooth(method="lm")
#+END_SRC

#+RESULTS:
[[file:fitresid1.png]]

- Our predictions aren't entirely noise, but we don't handle the tails well at all.

#+BEGIN_SRC R :results output graphics :exports results :file ppcheck1.png :width 600
pp_check(stanlm)
#+END_SRC

#+RESULTS:
[[file:ppcheck1.png]]

*pp_check* (from rstanarm) simulates 100 replications of Y given the model, and plots thea
results against the actual y. This provides a quick and useful diagnostic of the model and
whether or not it make sense. 

#+BEGIN_SRC R :exports both :results table :colnames yes
require(broom)
tidy(stanlm3)
#+END_SRC

#+RESULTS:
| term                 |           estimate |         std.error |
|----------------------+--------------------+-------------------|
| (Intercept)          |   11.3627285946104 | 0.154733297782236 |
| postal_codeDublin 10 |  -0.41109031117536 |  0.26829141909359 |
| postal_codeDublin 11 | -0.173802967126711 | 0.167608728709565 |
| postal_codeDublin 12 |  0.211707512805625 | 0.161083990692815 |
| postal_codeDublin 13 |  0.351751352766792 | 0.174571463002829 |
| postal_codeDublin 14 |  0.850544366948465 | 0.175828046070598 |
| postal_codeDublin 15 |  0.104697991708676 | 0.144349977022833 |
| postal_codeDublin 16 |  0.550930577885318 | 0.164866411221373 |
| postal_codeDublin 17 | -0.115676682206768 |   0.2504145301438 |
| postal_codeDublin 18 |  0.522518676746559 | 0.156919249177992 |
| postal_codeDublin 2  |   0.35072605038513 | 0.203699368253987 |
| postal_codeDublin 20 | 0.0145478008340139 | 0.285285555125346 |
| postal_codeDublin 22 |  0.100125034977552 | 0.185726468173443 |
| postal_codeDublin 24 | 0.0636090405878655 | 0.153543332710066 |
| postal_codeDublin 3  |  0.401971561031699 | 0.175861941946171 |
| postal_codeDublin 4  |  0.584983004574166 | 0.174661206704215 |
| postal_codeDublin 5  |  0.366239167558408 | 0.181184171309947 |
| postal_codeDublin 6  |  0.911124521428617 | 0.208110364717554 |
| postal_codeDublin 6w |  0.671847431154482 | 0.306615927075424 |
| postal_codeDublin 7  |  0.317970976360636 | 0.164814422402402 |
| postal_codeDublin 8  | 0.0360325510562647 | 0.163860965698879 |
| postal_codeDublin 9  |  0.316256275733578 | 0.167580242195925 |
| postal_code(Missing) |  0.281938411974061 | 0.130642614164143 |

* Some MCMC, for some reason

- Example taken from Appendix C, Bayesian Data Analysis (3rd Ed)

#+BEGIN_SRC stan :tangle schools.stan
data {
  int<lower=0> J; //num schools
  real y[J]; //estimated treatment effects
  real<lower=0> sigma[J]; //se effect estimates
}
parameters {
  real mu;
  real<lower=0> tau;
  vector[J] eta;

}
transformed parameters {
  vector[J] theta; //school effects
  theta = mu + tau*eta;
}
model {
  eta ~ normal(0, 1);
  y ~ normal(theta, sigma);
}
#+END_SRC
#+TABLE:schools
| school | estimate | sd |
| A      |       28 | 15 |
| B      |        8 | 10 |
| C      |       -3 | 16 |
| D      |        7 | 11 |
| E      |       -1 |  9 |
| F      |        1 | 11 |
| G      |       18 | 10 |
| H      |       12 | 18 |

#+BEGIN_SRC R :results none :exports code
schools <- read.csv("schools.csv")
J  <- nrow(schools)
y <- schools$estimate
sigma <- schools$sd
library(rstan)
schools_fit <- stan(file="schools.stan",
                    data=c("J", "y", "sigma"),
                    iter=2000, chains=4)
#+END_SRC
*  Property, Geocoded

- Shane Lynn did this before.

- We can read in his data and get a big head start

- only covers 2012-17

- but has latitude and longitude, and shapefiles, which will help a lot. 

#+BEGIN_SRC R :results none :exports code :eval maybe
require(sp)
require(rgdal)
require(tidyverse)
ppr_gc <- read_csv("ppr_geocoded_till_oct2018.csv")

ppr_gc_smaller <- select(ppr_gc, year, input_string, sale_date, price, ppr_county, geo_county, description_of_property, 15:24) %>% filter(price<1e6)
ppr_gc2 <- filter(ppr_gc_smaller, !is.na(latitude), !is.na(electoral_district))
locs <- select(ppr_gc2, longitude, latitude)
sp_ppr <- SpatialPointsDataFrame(locs, data=ppr_gc2, proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
shp <- readOGR("electoral_divisions_gps.shp")
dublin_counties <- c("Fingal", "Dn Laoghaire-Rathdown", "Dublin City", 
                     "South Dublin", "Kildare County", "Wicklow County")
dubcity <- "Dublin City"
subset <- shp[as.character(shp@data$COUNTYNAME) %in% 
              dublin_counties, ]
subset2 <- shp[as.character(shp@data$COUNTYNAME)=="Dublin City",]
mapdata <- fortify(subset)

dubcity <- filter(ppr_gc2, geo_county %in% dublin_counties)
dubcity_samp <- sample_frac(dubcity, size=0.3)


#+END_SRC

Lets start at the beginning again, and not assume that the data is
perfectly similar to the old one. 
#+BEGIN_SRC R :session :colnames yes
names(ppr_gc)
#+END_SRC

#+RESULTS:
| x                         |
|---------------------------|
| X1                        |
| year                      |
| input_string              |
| sale_date                 |
| address                   |
| postal_code               |
| ppr_county                |
| price                     |
| not_full_market_price     |
| vat_exclusive             |
| description_of_property   |
| property_size_description |
| date                      |
| formatted_address         |
| accuracy                  |
| latitude                  |
| longitude                 |
| postcode                  |
| type                      |
| geo_county                |
| electoral_district        |
| electoral_district_id     |
| region                    |
| small_area                |

The last twelve columns are new, and will definitely facilitate more granular analysis. 
We need to look at what date ranges this data covers
#+BEGIN_SRC R :colnames yes
ppr_gc %>% summarise(max=max(sale_date), min=min(sale_date))
#+END_SRC

#+RESULTS:
|        max |        min |
|------------+------------|
| 2018-10-12 | 2012-01-01 |

So it covers five years worth of data, and cuts off at end 2016.
Checking our previous dataset, we realise that there are about
94K sales since then. Assuming a limit of 2k calls per day,
we could do this in 47 days using Google's services. 
To be fair, that's not the end of the world, especially as 
it would be pretty simple to keep it updates afterwards. 


#+BEGIN_SRC R :exports code :results none
tenure <- read_csv("housing_tenure.csv") %>% normalise_names()
names(tenure) <- gsub("^_", "perc_", x=names(tenure))

ppr_tenure_m <- merge(ppr_gc, tenure, by.x="small_area", by.y="geog_id", all.x=TRUE)
ppr_tenure_less_1m <- filter(ppr_tenure_m, price<=1e6)
ppr_tenure_more_1m <- filter(ppr_tenure_m, price>1e6)
elec_price <- ppr_tenure_less_1m %>% group_by(electoral_district, electoral_district_id, year) %>% summarise(med_price=median(price), count=n())
elec_m <- merge(subset2, elec_price, by.x="CSOED", by.y="electoral_district_id", duplicateGeoms=TRUE)
elec_m_duball <- merge(subset, elec_price, by.x="CSOED", by.y="electoral_district_id", duplicateGeoms=TRUE)

elec_m_sf <- st_as_sf(elec_m)
elec_m_sf_duball <- st_as_sf(elec_m_duball)
elec_m_tenure <- merge(elec_m_sf, tenure, by.x="CSOED", by.y="ed_ward_id")
#this took more time than I expected. 
require(sf)
subset_sf <- st_as_sf(subset)
subset_sf2 <- mutate(subset_sf, PROP_UNOCC=UNOCC2011/HS2011, PROP_MALE=MALE2011/TOTAL2011, PROP_FEMALE=FEMALE2011/TOTAL2011, PER_PER_AREA=TOTAL2011/LAND_AREA, PEOPLE_PER_HS=TOTAL2011/HS2011)
#+END_SRC



If we convert the data to an *sf* object, we can examine patterns
by small area. 

#+BEGIN_SRC R :results output graphics :file unocc2011.png :exports both
plot(subset_sf2["PROP_UNOCC"])
#+END_SRC

#+RESULTS:
[[file:unocc2011.png]]

Not sure that makes a lot of sense, given that 
the areas close to the centre have way too many unoccupied dwellings. 

#+BEGIN_SRC R :results output graphics :file prop_male.png :exports results
plot(subset_sf2["PROP_MALE"])
#+END_SRC

#+RESULTS:
[[file:prop_male.png]]

For those seeking men. 


#+BEGIN_SRC R :results output graphics :file prop_female.png :exports results
plot(subset_sf2["PROP_FEMALE"])
#+END_SRC

#+RESULTS:
[[file:prop_female.png]]


At least I've written down how to use *sf* [fn:25]

Next step is to re-do the Googly dance to get dublin maps with features

This will make any further plots much more readable. 

#+BEGIN_SRC R :exports code :results none
require(ggmap)
key <- scan("key.txt", what="character")
register_google(key=key)
dubmap <- get_map("Dublin, Ireland", zoom=11)
#+END_SRC

#+BEGIN_SRC R :exports code :eval no
stan_elec_dub <- stan_lmer(log(price)~region+
                               (postcode|year)+
                               (elec_dist|postcode),
                           data=dubcity_samp2)
#+END_SRC

#+BEGIN_SRC python :tangle geocode_shane_lynn.py
"""
Python script for batch geocoding of addresses using the Google Geocoding API.
This script allows for massive lists of addresses to be geocoded for free by pausing when the 
geocoder hits the free rate limit set by Google (2500 per day).  If you have an API key for paid
geocoding from Google, set it in the API key section.
Addresses for geocoding can be specified in a list of strings "addresses". In this script, addresses
come from a csv file with a column "Address". Adjust the code to your own requirements as needed.
After every 500 successul geocode operations, a temporary file with results is recorded in case of 
script failure / loss of connection later.
Addresses and data are held in memory, so this script may need to be adjusted to process files line
by line if you are processing millions of entries.
Shane Lynn
5th November 2016
"""

import pandas as pd
import requests
import logging
import time

logger = logging.getLogger("root")
logger.setLevel(logging.DEBUG)
# create console handler
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)
logger.addHandler(ch)

#------------------ CONFIGURATION -------------------------------

# Set your Google API key here. 
# Even if using the free 2500 queries a day, its worth getting an API key since the rate limit is 50 / second.
# With API_KEY = None, you will run into a 2 second delay every 10 requests or so.
# With a "Google Maps Geocoding API" key from https://console.developers.google.com/apis/, 
# the daily limit will be 2500, but at a much faster rate.
# Example: API_KEY = 'AIzaSyC9azed9tLdjpZNjg2_kVePWvMIBq154eA'
# API_KEY = 'AIzaSyDqYnbecCHgRjpXZkVan1DQT9eXn_Fw56Q'
# Backoff time sets how many minutes to wait between google pings when your API limit is hit
BACKOFF_TIME = 30
# Set your output file name here.
output_filename = 'output_full2.csv'
# Set your input file here
input_filename = 'prop2017-18.csv'
# Specify the column name in your input data that contains addresses here
address_column_name = "address"
# Return Full Google Results? If True, full JSON results from Google are included in output
RETURN_FULL_RESULTS = True

#------------------ DATA LOADING --------------------------------

# Read the data to a Pandas Dataframe
data = pd.read_csv(input_filename, encoding='utf8')

if address_column_name not in data.columns:
	raise ValueError("Missing Address column in input data")

# Form a list of addresses for geocoding:
# Make a big list of all of the addresses to be processed.
addresses = data[address_column_name].tolist()

# **** DEMO DATA / IRELAND SPECIFIC! ****
# We know that these addresses are in Ireland, and there's a column for county, so add this for accuracy. 
# (remove this line / alter for your own dataset)
addresses = (data[address_column_name] + ',' + data['county'] + ',Ireland').tolist()


#------------------	FUNCTION DEFINITIONS ------------------------

def get_google_results(address, api_key=None, return_full_response=False):
    """
    Get geocode results from Google Maps Geocoding API.
    
    Note, that in the case of multiple google geocode reuslts, this function returns details of the FIRST result.
    
    @param address: String address as accurate as possible. For Example "18 Grafton Street, Dublin, Ireland"
    @param api_key: String API key if present from google. 
                    If supplied, requests will use your allowance from the Google API. If not, you
                    will be limited to the free usage of 2500 requests per day.
    @param return_full_response: Boolean to indicate if you'd like to return the full response from google. This
                    is useful if you'd like additional location details for storage or parsing later.
    """
    # Set up your Geocoding url
    geocode_url = "https://maps.googleapis.com/maps/api/geocode/json?address={}".format(address)
    if api_key is not None:
        geocode_url = geocode_url + "&key={}".format(api_key)
        
    # Ping google for the reuslts:
    results = requests.get(geocode_url)
    # Results will be in JSON format - convert to dict using requests functionality
    results = results.json()
    
    # if there's no results or an error, return empty results.
    if len(results['results']) == 0:
        output = {
            "formatted_address" : None,
            "latitude": None,
            "longitude": None,
            "accuracy": None,
            "google_place_id": None,
            "type": None,
            "postcode": None
        }
    else:    
        answer = results['results'][0]
        output = {
            "formatted_address" : answer.get('formatted_address'),
            "latitude": answer.get('geometry').get('location').get('lat'),
            "longitude": answer.get('geometry').get('location').get('lng'),
            "accuracy": answer.get('geometry').get('location_type'),
            "google_place_id": answer.get("place_id"),
            "type": ",".join(answer.get('types')),
            "postcode": ",".join([x['long_name'] for x in answer.get('address_components') 
                                  if 'postal_code' in x.get('types')])
        }
        
    # Append some other details:    
    output['input_string'] = address
    output['number_of_results'] = len(results['results'])
    output['status'] = results.get('status')
    if return_full_response is True:
        output['response'] = results
    
    return output

#------------------ PROCESSING LOOP -----------------------------

# Ensure, before we start, that the API key is ok/valid, and internet access is ok
test_result = get_google_results("London, England", API_KEY, RETURN_FULL_RESULTS)
if (test_result['status'] != 'OK') or (test_result['formatted_address'] != 'London, UK'):
    logger.warning("There was an error when testing the Google Geocoder.")
    raise ConnectionError('Problem with test results from Google Geocode - check your API key and internet connection.')

# Create a list to hold results
results = []
# Go through each address in turn
for address in addresses:
    # While the address geocoding is not finished:
    geocoded = False
    while geocoded is not True:
        # Geocode the address with google
        try:
            geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)
        except Exception as e:
            logger.exception(e)
            logger.error("Major error with {}".format(address))
            logger.error("Skipping!")
            geocoded = True
            
        # If we're over the API limit, backoff for a while and try again later.
        if geocode_result['status'] == 'OVER_QUERY_LIMIT':
            logger.info("Hit Query Limit! Backing off for a bit.")
            time.sleep(BACKOFF_TIME * 60) # sleep for 30 minutes
            geocoded = False
        else:
            # If we're ok with API use, save the results
            # Note that the results might be empty / non-ok - log this
            if geocode_result['status'] != 'OK':
                logger.warning("Error geocoding {}: {}".format(address, geocode_result['status']))
            logger.debug("Geocoded: {}: {}".format(address, geocode_result['status']))
            results.append(geocode_result)           
            geocoded = True

    # Print status every 100 addresses
    if len(results) % 100 == 0:
    	logger.info("Completed {} of {} address".format(len(results), len(addresses)))
            
    # Every 500 addresses, save progress to file(in case of a failure so you have something!)
    if len(results) % 500 == 0:
        pd.DataFrame(results).to_csv("{}_bak".format(output_filename))
        print("saved {r} results to file".format(r=len(results)))
    if len(results) % 10000 == 0:
            pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
            break
# All done
logger.info("Finished geocoding all addresses")
# Write the full results to csv using the pandas library.
pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
#+END_SRC


#+BEGIN_SRC python :session :tangle misc.py
def list_files(path, pattern):
    import os
    """A wrapper around os.listdir() which filters
    the output based on pattern. Returns a list"""
    return [x for x in os.listdir(path) if pattern in x]
#+END_SRC

#+RESULTS:
| yearlyprices.png | stan_dub_lm1.rds | ppcheck1.png | prices_county200518.png | PPR.org~ | dublin5.png | output_full2.csv | res.txt | PPR.pdf | prop_undone.csv | PPR-2010.csv | Constituency_Boundaries__Generalised_20m__OSi_National_Statuatory_Boundaries_.shp | prop_test.csv | schools.stan | Website-mortgage-approvals-time-series-Sep2018.xls | stanlm1pl1.png | PPR.Rnw~ | PPR.Rnw | electoral_divisions_gps.shp | electoral_divisions_gps.prj | fitresid1.png | output_test2.csv | __MACOSX | dubsalesdaily.png | small_areas_gps.qpj | train_sample2.csv | electoral-divisions-gps-projection.zip | PPR-2013.csv | small_areas_gps.dbf | stanlm3.rds | PPR-ALL.xlsx | PPR.html | PPR-2011.csv | small_areas_gps.prj | rcso | PPR-2012.csv | .~lock.Website-mortgage-approvals-time-series-Sep2018.xls# | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.shp | .git | ppr_data_encoded.csv | stanmodels1711.rda | PPR-plots.pdf | PPR.aux | meanyearly.png | geckodriver.log | mortgagevols.png | PPR-2017.csv | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.zip | .~lock.PPRAll2.csv# | PPR.org | PPR-addresses.csv | county2.png | small_areas_gps.shx | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.dbf | prices200518.png | small_areas_gps.shp | sex_age_pop_census_ed_2016.csv | gc_results.csv | geocoding-output-2012-2017.zip | PPRAll2.csv | train_sample.csv | PPR-2015.csv | ppr_data_encoded.csv.zip | .~lock.central_bank_mortgage_arrears_data.xlsx# | PPR.tex | mortgage_av_vals.png | .~lock.sex_age_pop_census_ed_2016.csv# | Constituency_Boundaries__Generalised_20m__OSi_National_Statuatory_Boundaries_.shx | output_full2.csv_bak | dubmonthly.png | Constituency_Boundaries__Generalised_20m__OSi_National_Statuatory_Boundaries_.zip | county1.png | central_bank_mortgage_arrears_data.xlsx | hist1.png | pricedens.png | schools.csv | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.shx | sample.csv | stanlm.rds | .~lock.yearly_property_prices.csv# | electoral_divisions_gps.qpj | Constituency_Boundaries__Generalised_20m__OSi_National_Statuatory_Boundaries_.prj | yearly_property_prices.csv | PPR-2014.csv | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.cpg | Constituency_Boundaries__Generalised_20m__OSi_National_Statuatory_Boundaries_.dbf | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.prj | geocode.py | output_test.csv | county3.png | .~lock.PPR-addresses.csv# | geocode_shane_lynn.py | Constituency_Boundaries__Generalised_20m__OSi_National_Statuatory_Boundaries_.cpg | stanlm4.rds | #PPR.Rnw# | output_full.csv_bak | dublin1.png | yearly.png | richitets.txt | .Rhistory | prop2017-18.csv | dub_areas.png | small-areas-gps-projection.zip | electoral_divisions_gps.dbf | dublin2.png | electoral_divisions_gps.shx | PPR-2016.csv | mortgagecount.png | stanlm_dub.rds | Small_Areas_Generalised_20m__OSi_National_Statistical_Boundaries.xml | PPR.log | auto | README.org | PPR.out | stanmods201118.rda | dublin3.png | dublin6.png | mortgagevals.png | PPR.toc | stanlm2.rds | countyearly.png |

The above is a simple wrapper around os.listdir() which 
returns results matching a pattern.
I like the use of a list comp here, its nice
The in operator is particularly useful. 

I can just hash the addresses to not return previously
processed results. 

but i could also use locality-sensitive hashes, which are pretty 
rocking for approximate matching.

The package *datasketch* has an implementation,
and I provide example code below. 

#+BEGIN_SRC python :session
from datasketch import MinHash, MinHashLSH

set1 = set(['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',
            'estimating', 'the', 'similarity', 'between', 'datasets'])
set2 = set(['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',
            'estimating', 'the', 'similarity', 'between', 'documents'])
set3 = set(['minhash', 'is', 'probability', 'data', 'structure', 'for',
            'estimating', 'the', 'similarity', 'between', 'documents'])

m1 = MinHash(num_perm=128)
m2 = MinHash(num_perm=128)
m3 = MinHash(num_perm=128)
for d in set1:
    m1.update(d.encode('utf8'))
for d in set2:
    m2.update(d.encode('utf8'))
for d in set3:
    m3.update(d.encode('utf8'))

# Create LSH index
lsh = MinHashLSH(threshold=0.9)
lsh.insert("m2", m2)
lsh.insert("m3", m3)
result = lsh.query(m1)
print("Approximate neighbours with Jaccard similarity > 0.5", result)
#+END_SRC


#+BEGIN_SRC python :session
import pandas as pd
from datasketch import MinHash, MinHashLSH
results = pd.read_csv("output_full2.csv_bak")
firstadd = results.formatted_address.head()
def address_to_set(address):
    """Takes an address a converts it to a set of words,
    space separated and lower-cased"""
    lowered = address.lower()
    splitted = lowered.split("\s+")
    set_split = set(splitted)
    return set_split

def create_minhash(address_set):
    """Takes the output of address_to_set and inserts
    each word into a minhash, returning this minhash"""
    m1 = MinHash()
    for word in address_set:
        m1.update(word.encode('utf8'))
    return m1

def add_to_minhash_set(minhashes, threshold=0.9):
    """Takes an dict of address, minihash pairs, and adds them
    to a new MinHashLSH object, which is returned"""
    lsh = MinHashLSH(threshold)
    for name, minhash in minhashes.items():
        lsh.insert(name, minhash)
    return lsh

def query_from_minhash_set(text, lsh):
    add_dict = {add : create_minhash(address_to_set(add))
                for add in text}
    results = []
    for name, hash in add_dict.items():
        results.append((name, lsh.query(hash)))
    return results

firstadd = results.formatted_address.head()
oneadd= firstadd[0]
s1 = address_to_set(oneadd)
mh1 = create_minhash(s1)
addresses = results.formatted_address.fillna('')
allmh1 = [create_minhash(address_to_set(address)) for address in addresses]
firstadd_set = {add : create_minhash(address_to_set(add)) for add in firstadd}
set_test = add_to_minhash_set(firstadd_set)

alladd_set =  {add : create_minhash(address_to_set(add)) for add in addresses}
alladd_lsh = add_to_minhash_set(alladd_set)
q_test = query_from_minhash_set(firstadd, alladd_lsh_loose)
q_test2 = query_from_minhash_set(firstadd, alladd_lsh)
#+END_SRC


So I have an (apparently) working implementation.
It actually doesn't seem that useful though, as it merely
returns exact results.
To be fair, if I feed it more addresses it will probably do better
but it doesn't look like this will be particularly useful.
